---
layout: about
# title: about
permalink: /
# subtitle: <b>PhD student at EPFL ðŸ‡¨ðŸ‡­ 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circulare
  address: Enjoying the gorgeous ðŸ‡¨ðŸ‡­ peaks! This one is <a href="https://en.wikipedia.org/wiki/Rochers_de_Naye">Rochers de Naye</a>. #>
    # <p>EPFL, Lausanne, ðŸ‡¨ðŸ‡­</p>
    

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---


**[`Email`](mailto:maksym@andriushchenko.me)** &emsp; 
**[`Twitter/X`](https://twitter.com/maksym_andr)** &emsp; 
**[`Google Scholar`](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ)** &emsp; 
**[`GitHub`](https://github.com/max-andr)** &emsp; 
**[`CV`](cv.pdf)**

<!-- github_username: max-andr # your GitHub user name
# gitlab_username: # your GitLab user name
twitter_username: maksym_andr # your Twitter handle
linkedin_username: maksym-andriushchenko # your LinkedIn user name
scholar_userid: ZNtuJYoAAAAJ -->

**Short bio.** I work as a researcher at EPFL and consult for [Gray Swan AI](https://www.grayswan.ai/). I have worked on AI safety with leading organizations in the field (OpenAI, Anthropic, UK AI Safety Institute, Center for AI Safety). I obtained a PhD in machine learning from EPFL in 2024 advised by [Prof. Nicolas Flammarion](https://people.epfl.ch/nicolas.flammarion). My PhD thesis was awarded with the Patrick Denantes Memorial Prize for the best thesis in the CS department of EPFL and was supported by the [Google](https://research.google/outreach/phd-fellowship/recipients/) and [Open Phil AI](https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/) PhD Fellowships. I did my MSc at Saarland University and the University of TÃ¼bingen, and interned at Adobe Research. My full publication list is available [here](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ).
<!-- My current research mainly focuses on developing a better understanding of the training dynamics of deep networks and their adversarial robustness.  -->
<!-- **Formal bio.** Maksym Andriushchenko is a fourth-year PhD student in computer science at EPFL (Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne) in Switzerland. He obtained his MSc from Saarland University, Germany. His research mainly focuses on how to better understand the training dynamics of deep networks and make machine learning algorithms adversarially robust. Maksym has published eleven papers at major machine learning and computer vision conferences (NeurIPS, ICML, ICLR, CVPR, ECCV, etc). His research is supported by the Google and OpenPhil PhD Fellowships. -->

**I am on the faculty job market this 2024-2025 academic year. If you think my background can be a good fit for your department, please let me know.**


<!-- **Research interests.** -->
<!-- My primary research goal is to understand generalization in deep learning. I'm interested in the training dynamics of commonly used algorithms (e.g., [SGD with large step sizes](https://arxiv.org/abs/2210.05337), [sharpness-aware minimization](https://arxiv.org/abs/2206.06232), [fine-tuning language models](https://arxiv.org/abs/2006.04884)), adversarial robustness ([formal guarantees](https://arxiv.org/abs/1705.08475), [square attack](https://arxiv.org/abs/1912.00049), [fast adversarial training](https://arxiv.org/abs/2007.02617), [RobustBench](https://arxiv.org/abs/2010.09670)), and out-of-distribution generalization ([curious ReLU properties](https://arxiv.org/abs/1812.05720), generalization to image [corruptions](https://arxiv.org/abs/2103.02325) and [digital manipulations](https://arxiv.org/abs/2202.12860)).  -->
<!-- My primary research goal is to *understand generalization in deep learning*. Towards this goal, I've worked on adversarial robustness, out-of-distribution generalization, implicit regularization, and sharpness-aware minimization. These days, I'm looking more into optimization and generalization properties of language models. My full publication list is available [here](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ). -->
<!-- My primary research goal is to understand robustness and generalization in deep learning. Toward this goal, I've worked on adversarial robustness, out-of-distribution generalization, and implicit regularization. These days, I'm focusing entirely on robustness and alignment of large language models. My complete publication list is available [here](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ). -->
<!-- I'm interested in alignment, safety, and generalization of LLMs and AI agents. 


<!-- **On Ukraine.** Since I'm from Ukraine, I'm often asked about the situation in my country and how one can help. The most effective way is to donate to *local Ukrainian organization helping on the ground*, e.g., see [this list](https://standforukraine.com/) which includes both trusted military and humanitarian organizations. You can also host displaced scholars and students from Ukraine, e.g., see the [#ScienceForUkraine project](https://scienceforukraine.eu/) where I'm involved as a volunteer. You can also help simply by spreading the word about the war and going to demonstrations in your city. It's very important that we don't normalize [annexations of territories](https://en.wikipedia.org/wiki/2022_annexation_referendums_in_Russian-occupied_Ukraine), [numerous war crimes](https://en.wikipedia.org/wiki/War_crimes_in_the_2022_Russian_invasion_of_Ukraine), [mass deportations](https://theconversation.com/ukraine-war-reports-of-mass-deportations-recall-russias-dark-history-of-forcible-relocations-190272), and [nuclear threats](https://www.theatlantic.com/newsletters/archive/2022/09/russias-nuclear-threats/671571/). Otherwise, we'll end up in a world we don't really want to be in. -->


<!-- ## highlight -->

<!-- Check our ICML'22 paper -->
<!-- ![sam](./assets/img/publication_preview/sam_paper.png) -->
<!-- <div style="text-align: center;">
  <img src="./assets/img/publication_preview/sam_paper.png" alt="SAM slide" width="75%"/>
</div> -->



## selected publications

**M. Andriushchenko**, A. Souly, M. Dziemian, D. Duenas, M. Lin, J. Wang, D. Hendrycks, A. Zou, Z. Kolter, M. Fredrikson, E. Winsor, J. Wynne, Y. Gal, X. Davies. [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024) (arXiv, Oct 2024)

<!-- A. Zou, L. Phan, J. Wang, D. Duenas, M. Lin, **M. Andriushchenko**, R. Wang, Z. Kolter, M. Fredrikson, D. Hendrycks. [Improving Alignment and Robustness with Short Circuiting](https://arxiv.org/abs/2406.04313) (NeurIPS 2024) -->

**M. Andriushchenko**, F. Croce, N. Flammarion. [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) (ICML 2024 Workshop on the Next Generation of AI Safety)

<!-- P. Chao\*, E. Debenedetti\*, A. Robey\*, **M. Andriushchenko\***, F. Croce, V. Sehwag, E. Dobriban, N. Flammarion, G.J. Pappas, F. Tramer, H. Hassani, E. Wong. [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318) (arXiv, Apr 2024) -->

**M. Andriushchenko**, N. Flammarion. [Towards Understanding Sharpness-Aware Minimization](https://arxiv.org/abs/2206.06232) (ICML 2022)

F. Croce\*, **M. Andriushchenko\***, V. Sehwag\*, E. Debenedetti\*, N. Flammarion, M. Chiang, P. Mittal, M. Hein. [RobustBench: a standardized adversarial robustness benchmark](https://arxiv.org/abs/2010.09670) (NeurIPS 2021 Datasets and Benchmarks Track, Best Paper Honorable Mention Prize at ICLR'21 Workshop on Security and Safety in ML Systems)

**M. Andriushchenko\***, F. Croce\*, N. Flammarion, M. Hein. [Square Attack: a query-efficient black-box adversarial attack via random search](https://arxiv.org/abs/1912.00049) (ECCV 2020)

