---
layout: about
# title: about
permalink: /
# subtitle: <b>PhD student at EPFL üá®üá≠ 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circulare
  address: Enjoying the gorgeous üá®üá≠ peaks! This one is <a href="https://en.wikipedia.org/wiki/Rochers_de_Naye">Rochers de Naye</a>. #>
    # <p>EPFL, Lausanne, üá®üá≠</p>
    

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---


**[`Email`](mailto:maksym@andriushchenko.me)** &emsp; 
**[`Twitter/X`](https://twitter.com/maksym_andr)** &emsp; 
**[`Google Scholar`](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ)** &emsp; 
**[`GitHub`](https://github.com/max-andr)** &emsp; 
**[`CV`](cv.pdf)**


üëã **Short bio.** I am a principal investigator at the [ELLIS Institute T√ºbingen](https://institute-tue.ellis.eu/) and the [Max Planck Institute for Intelligent Systems](https://is.mpg.de/), where I lead the AI Safety and Alignment group. I also serve as a chapter lead for the new edition of the [International AI Safety Report](https://arxiv.org/abs/2501.17805) chaired by Yoshua Bengio. I have worked on AI safety with leading organizations in the field (OpenAI, Anthropic, UK AI Safety Institute, Center for AI Safety, Gray Swan AI). I obtained my PhD in machine learning from EPFL in 2024 advised by [Prof. Nicolas Flammarion](https://people.epfl.ch/nicolas.flammarion). My PhD thesis was awarded the Patrick Denantes Memorial Prize for the best thesis in the CS department of EPFL and was supported by the [Google](https://research.google/outreach/phd-fellowship/recipients/) and [Open Phil AI](https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/) PhD Fellowships. 
<!-- I completed my MSc at Saarland University and the University of T√ºbingen, and interned at Adobe Research.  -->

üì£ **I am hiring at all levels (bachelor's, master's, PhD, postdocs)!** I'm looking particularly for PhD students: both those able to start in Fall 2025 (i.e., as soon as possible) and through centralized programs like [CLS](https://learning-systems.org/), [ELLIS](https://ellis.eu/phd-postdoc), [IMPRS-IS](https://imprs.is.mpg.de/) (the deadlines are in November) to start in Spring‚ÄìFall 2026. **If you are interested, please fill out [this Google form](https://forms.gle/uu1UrN8RQrSy8wUk8). I will review every application and will reach out if there is a good fit.**

üîç **Research topics.** We focus on developing algorithmic solutions to reduce harms from advanced general-purpose AI models. We're particularly interested in alignment of autonomous LLM agents, which are becoming increasingly capable and pose a variety of emerging risks. We're also interested in rigorous AI evaluations and informing the public about the risks and capabilities of frontier AI models. Additionally, we aim to advance our understanding of how AI models generalize, which is crucial for ensuring their steerability and reducing associated risks. For more information about research topics relevant to our group, please check the following documents: [International AI Safety Report](https://arxiv.org/abs/2501.17805), [An Approach to Technical AGI Safety and Security by DeepMind](https://arxiv.org/abs/2504.01849), [Open Philanthropy‚Äôs 2025 RFP for Technical AI Safety Research](https://www.openphilanthropy.org/tais-rfp-research-areas/).

üìù **Research style.** We are not necessarily interested in getting X papers accepted at NeurIPS/ICML/ICLR. We are interested in making an impact: this *can* be papers (and NeurIPS/ICML/ICLR are great venues), but also open-source repositories, benchmarks, blog posts, even social media posts‚Äîliterally anything that can be genuinely useful for other researchers and the general public. For example, our JailbreakBench and AgentHarm benchmarks were not only published at NeurIPS and ICLR but also used by [DeepMind](https://arxiv.org/abs/2403.05530), [xAI](https://data.x.ai/2025-08-20-grok-4-model-card.pdf), and [Anthropic / UK AI Safety Institute](https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf) for evaluation of their new frontier LLMs. 

üåü **Broader vision.** Current machine learning methods are fundamentally different from what they used to be pre-2022. [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) summarized and predicted this shift very well back in 2019: *"general methods that leverage computation are ultimately the most effective"*. Taking this into account, we are only interested in studying methods that are general and scale with intelligence and compute. Everything that helps to advance their safety and alignment with societal values is relevant to us. We believe getting this‚Äîsome may call it "AGI"‚Äîright is one of the most important challenges of our time. **Join us on this journey!**

<!-- üë• **Current group members**:  -->
## AI Safety and Alignment Group

Current group members:
- [Alexander Panfilov](https://kotekjedi.github.io/) (PhD student, main advisor: [Jonas Geiping](https://jonasgeiping.github.io/))
- [Ben Rank](https://www.linkedin.com/in/ben-rank) (PhD student)
- [David Schmotz](https://www.linkedin.com/in/david-schmotz-840660150/) (PhD student)

Additionally, I've had the pleasure to supervise the following students: 
- Joshua Freeman (master's project at ETH Zurich ‚Üí SWE Intern at Meta)
- Hao Zhao (master's thesis at EPFL ‚Üí PhD student at EPFL)
- Hichem Hadhri (master's project at EPFL ‚Üí Data Science Intern at Swisscom)
- Tiberiu Musat (bachelor's project at EPFL ‚Üí MSc student at ETH Zurich)
- Francesco d'Angelo (PhD project at EPFL ‚Üí PhD student at EPFL, Google PhD Fellowship)
- Th√©au Vannier (master's project at EPFL ‚Üí Research Engineer at InstaDeep)
- Jana Vuckovic (master's project at EPFL ‚Üí Data Science Intern at Credit Suisse)
- Mehrdad Saberi (Summer@EPFL intern ‚Üí PhD student at University of Maryland)
- Edoardo Debenedetti (master's project at EPFL ‚Üí PhD student at ETH Zurich)
- Klim Kireev (PhD project at EPFL ‚Üí PhD student at EPFL, researcher at MPI-SP)
- Etienne Bonvin (master's project at EPFL ‚Üí Security Engineer at Global ID SA)
- Oriol Barbany (master's project at EPFL ‚Üí PhD student at UPC and EPFL)

<!-- üßë‚Äçüéì **Students.** I have supervised 13 students from EPFL and other universities. Their work has been accepted at top-tier conferences (such as [NeurIPS](https://arxiv.org/abs/2010.09670) and [ICML](https://arxiv.org/abs/2402.04833)), received academic recognition (Best Paper Honorable Mention Prize at an [ICLR Workshop](https://aisecure-workshop.github.io/aml-iclr2021/), a [nomination](https://marcelluszhao.github.io/) for EPFL Outstanding Master's Thesis), and has been featured in [press](https://www.mittrchina.com/news/detail/13848).  -->
<!-- Alexander Panfilov (University of T√ºbingen; co-supervising with Jonas Geiping within the ELLIS PhD program), Joshua Freeman (ETH), Hao Zhao (EPFL), Hichem Hadhri (EPFL), Tiberiu Musat (EPFL), Francesco d'Angelo (EPFL), Th√©au Vannier (EPFL), Jana Vuckovic (EPFL), Mehrdad Saberi (EPFL), Edoardo Debenedetti (EPFL), Klim Kireev (EPFL), Etienne Bonvin (EPFL), Oriol Barbany (EPFL). -->

<!-- üè≠ **Industry impact.** Our recent LLM benchmarks were used by [the Gemini 1.5 team](https://arxiv.org/abs/2403.05530) ([JailbreakBench](https://arxiv.org/abs/2404.01318)) and by [the US/UK AI Safety Institutes](https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf) for pre-deployment testing of Claude 3.5 Sonnet ([AgentHarm](https://arxiv.org/abs/2410.09024)). In addition, I have participated in red teaming of models and services from OpenAI as an independent contributor and from Anthropic via Gray Swan AI. At EPFL, beyond my PhD fellowships, I helped write four successful grant proposals funded by Google and Schmidt Sciences ($450,000 in total). During my internship at Adobe Research in Summer 2021, I worked on enhancing the adversarial robustness of [content provenance models](https://arxiv.org/abs/2202.12860) to address deepfakes. -->


<!-- **Research interests.** -->
<!-- My primary research goal is to understand generalization in deep learning. I'm interested in the training dynamics of commonly used algorithms (e.g., [SGD with large step sizes](https://arxiv.org/abs/2210.05337), [sharpness-aware minimization](https://arxiv.org/abs/2206.06232), [fine-tuning language models](https://arxiv.org/abs/2006.04884)), adversarial robustness ([formal guarantees](https://arxiv.org/abs/1705.08475), [square attack](https://arxiv.org/abs/1912.00049), [fast adversarial training](https://arxiv.org/abs/2007.02617), [RobustBench](https://arxiv.org/abs/2010.09670)), and out-of-distribution generalization ([curious ReLU properties](https://arxiv.org/abs/1812.05720), generalization to image [corruptions](https://arxiv.org/abs/2103.02325) and [digital manipulations](https://arxiv.org/abs/2202.12860)).  -->
<!-- My primary research goal is to *understand generalization in deep learning*. Towards this goal, I've worked on adversarial robustness, out-of-distribution generalization, implicit regularization, and sharpness-aware minimization. These days, I'm looking more into optimization and generalization properties of language models. My full publication list is available [here](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ). -->
<!-- My primary research goal is to understand robustness and generalization in deep learning. Toward this goal, I've worked on adversarial robustness, out-of-distribution generalization, and implicit regularization. These days, I'm focusing entirely on robustness and alignment of large language models. My complete publication list is available [here](https://scholar.google.com/citations?user=ZNtuJYoAAAAJ). -->
<!-- I'm interested in alignment, safety, and generalization of LLMs and AI agents. 


<!-- **On Ukraine.** Since I'm from Ukraine, I'm often asked about the situation in my country and how one can help. The most effective way is to donate to *local Ukrainian organization helping on the ground*, e.g., see [this list](https://standforukraine.com/) which includes both trusted military and humanitarian organizations. You can also host displaced scholars and students from Ukraine, e.g., see the [#ScienceForUkraine project](https://scienceforukraine.eu/) where I'm involved as a volunteer. You can also help simply by spreading the word about the war and going to demonstrations in your city. It's very important that we don't normalize [annexations of territories](https://en.wikipedia.org/wiki/2022_annexation_referendums_in_Russian-occupied_Ukraine), [numerous war crimes](https://en.wikipedia.org/wiki/War_crimes_in_the_2022_Russian_invasion_of_Ukraine), [mass deportations](https://theconversation.com/ukraine-war-reports-of-mass-deportations-recall-russias-dark-history-of-forcible-relocations-190272), and [nuclear threats](https://www.theatlantic.com/newsletters/archive/2022/09/russias-nuclear-threats/671571/). Otherwise, we'll end up in a world we don't really want to be in. -->


<!-- ## highlight -->

<!-- Check our ICML'22 paper -->
<!-- ![sam](./assets/img/publication_preview/sam_paper.png) -->
<!-- <div style="text-align: center;">
  <img src="./assets/img/publication_preview/sam_paper.png" alt="SAM slide" width="75%"/>
</div> -->



## selected publications

T. Kuntz, A. Duzan, H. Zhao, F. Croce, Z. Kolter, N. Flammarion, **M. Andriushchenko**. [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866) (NeurIPS 2025 Datasets and Benchmarks Track, **Spotlight**)

**M. Andriushchenko**, A. Souly, M. Dziemian, D. Duenas, M. Lin, J. Wang, D. Hendrycks, A. Zou, Z. Kolter, M. Fredrikson, E. Winsor, J. Wynne, Y. Gal, X. Davies. [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024) (ICLR 2025)

**M. Andriushchenko**, F. Croce, N. Flammarion. [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) (ICLR 2025)

A. Zou, L. Phan, J. Wang, D. Duenas, M. Lin, **M. Andriushchenko**, R. Wang, Z. Kolter, M. Fredrikson, D. Hendrycks. [Improving Alignment and Robustness with Circuit Breakers](https://arxiv.org/abs/2406.04313) (NeurIPS 2024)

<!-- P. Chao\*, E. Debenedetti\*, A. Robey\*, **M. Andriushchenko\***, F. Croce, V. Sehwag, E. Dobriban, N. Flammarion, G.J. Pappas, F. Tramer, H. Hassani, E. Wong. [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318) (arXiv, Apr 2024) -->

<!-- **M. Andriushchenko**, N. Flammarion. [Towards Understanding Sharpness-Aware Minimization](https://arxiv.org/abs/2206.06232) (ICML 2022) -->

<!-- F. Croce\*, **M. Andriushchenko\***, V. Sehwag\*, E. Debenedetti\*, N. Flammarion, M. Chiang, P. Mittal, M. Hein. [RobustBench: a standardized adversarial robustness benchmark](https://arxiv.org/abs/2010.09670) (NeurIPS 2021 Datasets and Benchmarks Track, Best Paper Honorable Mention Prize at ICLR'21 Workshop on Security and Safety in ML Systems) -->

<!-- **M. Andriushchenko\***, F. Croce\*, N. Flammarion, M. Hein. [Square Attack: a query-efficient black-box adversarial attack via random search](https://arxiv.org/abs/1912.00049) (ECCV 2020) -->

