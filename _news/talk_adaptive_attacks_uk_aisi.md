---
layout: post
date: 2024-11-04 
inline: true
---

An invited talk at the UK AI Safety Institute about [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151), where we achieved 100% jailbreak success rate on all major LLMs, including GPT-4o and Claude 3.5 Sonnet.