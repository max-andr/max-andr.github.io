<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Maksym Andriushchenko</title> <meta name="author" content="Maksym Andriushchenko"/> <meta name="description" content="I'm a PhD student in computer science at EPFL advised by Nicolas Flammarion. I'm interested in understanding why machine learning works and why it fails. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>📖</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://andriushchenko.me/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/"><span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Maksym Andriushchenko </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> Enjoying the gorgeous 🇨🇭 peaks! This one is <a href="https://en.wikipedia.org/wiki/Rochers_de_Naye" target="_blank" rel="noopener noreferrer">Rochers de Naye</a>. </div> </div> <div class="clearfix"> <p><strong><a href="mailto:maksym@andriushchenko.me"><code class="language-plaintext highlighter-rouge">Email</code></a></strong>   <strong><a href="https://aisagroup.substack.com/" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">Substack</code></a></strong>   <strong><a href="https://x.com/maksym_andr" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">X</code></a></strong>   <strong><a href="https://scholar.google.com/citations?user=ZNtuJYoAAAAJ" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">Scholar</code></a></strong>   <strong><a href="https://github.com/max-andr" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">GitHub</code></a></strong>   <strong><a href="cv.pdf"><code class="language-plaintext highlighter-rouge">CV</code></a></strong></p> <p>👋 <strong>Short bio.</strong> I am a principal investigator at the <a href="https://institute-tue.ellis.eu/" target="_blank" rel="noopener noreferrer">ELLIS Institute Tübingen</a> and the <a href="https://is.mpg.de/" target="_blank" rel="noopener noreferrer">Max Planck Institute for Intelligent Systems</a>, where I lead the AI Safety and Alignment group. I also serve as chapter lead for the new edition of the <a href="https://arxiv.org/abs/2501.17805" target="_blank" rel="noopener noreferrer">International AI Safety Report</a> chaired by Yoshua Bengio. I have worked on AI safety with leading organizations in the field (OpenAI, Anthropic, UK AI Safety Institute, Center for AI Safety, Gray Swan AI). I obtained my PhD in machine learning from EPFL in 2024 advised by <a href="https://people.epfl.ch/nicolas.flammarion" target="_blank" rel="noopener noreferrer">Prof. Nicolas Flammarion</a>. My PhD thesis was awarded the Patrick Denantes Memorial Prize for the best thesis in the CS department of EPFL and was supported by the <a href="https://research.google/outreach/phd-fellowship/recipients/" target="_blank" rel="noopener noreferrer">Google</a> and <a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/" target="_blank" rel="noopener noreferrer">Open Phil AI</a> PhD Fellowships. </p> <p>📣 <strong>I’m hiring!</strong> I’m looking for one postdoc, multiple PhD students (apply to <a href="https://learning-systems.org/" target="_blank" rel="noopener noreferrer">CLS</a>, <a href="https://ellis.eu/phd-postdoc" target="_blank" rel="noopener noreferrer">ELLIS</a>, <a href="https://imprs.is.mpg.de/" target="_blank" rel="noopener noreferrer">IMPRS-IS</a> by November 2025 to start in Spring–Fall 2026), and multiple master’s thesis students. <strong>If you are interested, please fill out <a href="https://forms.gle/uu1UrN8RQrSy8wUk8" target="_blank" rel="noopener noreferrer">this Google form</a>. I will review every application and will reach out if there is a good fit.</strong></p> <p>🔍 <strong>Research topics.</strong> We focus on developing algorithmic solutions to reduce harms from advanced general-purpose AI models. We’re particularly interested in alignment of autonomous LLM agents, which are becoming increasingly capable and pose a variety of emerging risks. We’re also interested in rigorous AI evaluations and informing the public about the risks and capabilities of frontier AI models. Additionally, we aim to advance our understanding of how AI models generalize, which is crucial for ensuring their steerability and reducing associated risks. For more information about research topics relevant to our group, please check the following documents: <a href="https://arxiv.org/abs/2501.17805" target="_blank" rel="noopener noreferrer">International AI Safety Report</a>, <a href="https://arxiv.org/abs/2504.01849" target="_blank" rel="noopener noreferrer">An Approach to Technical AGI Safety and Security by DeepMind</a>, <a href="https://www.openphilanthropy.org/tais-rfp-research-areas/" target="_blank" rel="noopener noreferrer">Open Philanthropy’s 2025 RFP for Technical AI Safety Research</a>.</p> <p>📝 <strong>Research style.</strong> We are not necessarily interested in getting X papers accepted at NeurIPS/ICML/ICLR. We are interested in making an impact: this <em>can</em> be papers (and NeurIPS/ICML/ICLR are great venues), but also open-source repositories, benchmarks, blog posts, even social media posts—literally anything that can be genuinely useful for other researchers and the general public. For example, our JailbreakBench and AgentHarm benchmarks were not only published at NeurIPS and ICLR but also used by <a href="https://arxiv.org/abs/2403.05530" target="_blank" rel="noopener noreferrer">DeepMind</a>, <a href="https://data.x.ai/2025-08-20-grok-4-model-card.pdf" target="_blank" rel="noopener noreferrer">xAI</a>, and <a href="https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf" target="_blank" rel="noopener noreferrer">Anthropic / UK AI Safety Institute</a> for evaluation of their new frontier LLMs.</p> <p>🌟 <strong>Broader vision.</strong> Current machine learning methods are fundamentally different from what they used to be pre-2022. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank" rel="noopener noreferrer">The Bitter Lesson</a> summarized and predicted this shift very well back in 2019: <em>“general methods that leverage computation are ultimately the most effective”</em>. Taking this into account, we are only interested in studying methods that are general and scale with intelligence and compute. Everything that helps to advance their safety and alignment with societal values is relevant to us. We believe getting this—some may call it “AGI”—right is one of the most important challenges of our time. <strong>Join us on this journey!</strong></p> <h2 id="ai-safety-and-alignment-group">AI Safety and Alignment Group</h2> <p>Current group members:</p> <ul> <li> <a href="https://kotekjedi.github.io/" target="_blank" rel="noopener noreferrer">Alexander Panfilov</a> (PhD student, main advisor: <a href="https://jonasgeiping.github.io/" target="_blank" rel="noopener noreferrer">Jonas Geiping</a>)</li> <li> <a href="https://www.linkedin.com/in/ben-rank" target="_blank" rel="noopener noreferrer">Ben Rank</a> (PhD student)</li> <li> <a href="https://www.linkedin.com/in/david-schmotz-840660150/" target="_blank" rel="noopener noreferrer">David Schmotz</a> (PhD student)</li> </ul> <p>Additionally, I’ve had the pleasure to supervise the following students:</p> <ul> <li>Joshua Freeman (master’s project at ETH Zurich → SWE Intern at Meta)</li> <li>Hao Zhao (master’s thesis at EPFL → PhD student at EPFL)</li> <li>Hichem Hadhri (master’s project at EPFL → Data Science Intern at Swisscom)</li> <li>Tiberiu Musat (bachelor’s project at EPFL → MSc student at ETH Zurich)</li> <li>Francesco d’Angelo (PhD project at EPFL → PhD student at EPFL, Google PhD Fellowship)</li> <li>Théau Vannier (master’s project at EPFL → Research Engineer at InstaDeep)</li> <li>Jana Vuckovic (master’s project at EPFL → Data Science Intern at Credit Suisse)</li> <li>Mehrdad Saberi (Summer@EPFL intern → PhD student at University of Maryland)</li> <li>Edoardo Debenedetti (master’s project at EPFL → PhD student at ETH Zurich)</li> <li>Klim Kireev (PhD project at EPFL → PhD student at EPFL, researcher at MPI-SP)</li> <li>Etienne Bonvin (master’s project at EPFL → Security Engineer at Global ID SA)</li> <li>Oriol Barbany (master’s project at EPFL → PhD student at UPC and EPFL)</li> </ul> <h2 id="selected-publications">selected publications</h2> <p>T. Kuntz, A. Duzan, H. Zhao, F. Croce, Z. Kolter, N. Flammarion, <strong>M. Andriushchenko</strong>. <a href="https://arxiv.org/abs/2506.14866" target="_blank" rel="noopener noreferrer">OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents</a> (NeurIPS 2025 Datasets and Benchmarks Track, <strong>Spotlight</strong>)</p> <p><strong>M. Andriushchenko</strong>, A. Souly, M. Dziemian, D. Duenas, M. Lin, J. Wang, D. Hendrycks, A. Zou, Z. Kolter, M. Fredrikson, E. Winsor, J. Wynne, Y. Gal, X. Davies. <a href="https://arxiv.org/abs/2410.09024" target="_blank" rel="noopener noreferrer">AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</a> (ICLR 2025)</p> <p><strong>M. Andriushchenko</strong>, F. Croce, N. Flammarion. <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</a> (ICLR 2025)</p> <p>A. Zou, L. Phan, J. Wang, D. Duenas, M. Lin, <strong>M. Andriushchenko</strong>, R. Wang, Z. Kolter, M. Fredrikson, D. Hendrycks. <a href="https://arxiv.org/abs/2406.04313" target="_blank" rel="noopener noreferrer">Improving Alignment and Robustness with Circuit Breakers</a> (NeurIPS 2024)</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 19, 2025</th> <td> Check out three new papers: <a href="https://arxiv.org/abs/2505.20162" target="_blank" rel="noopener noreferrer">Capability-Based Scaling Laws for LLM Red-Teaming</a>, <a href="https://arxiv.org/abs/2506.10949" target="_blank" rel="noopener noreferrer">Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors</a>, and <a href="https://arxiv.org/abs/2506.14866" target="_blank" rel="noopener noreferrer">OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents</a>! </td> </tr> <tr> <th scope="row">Mar 29, 2025</th> <td> <strong>An invited talk at the <a href="https://lmxsafety.com/2025/index.html" target="_blank" rel="noopener noreferrer">Large Model Safety Workshop</a> (April 23) collocated with ICLR 2025 in Singapore. Honored to present my work alongside so many amazing speakers!</strong> </td> </tr> <tr> <th scope="row">Feb 1, 2025</th> <td> <strong>Our works on LLM safety (<a href="https://arxiv.org/abs/2406.04313" target="_blank" rel="noopener noreferrer">circuit breakers</a> and <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">adaptive jailbreak attacks</a>) have been featured in <a href="https://www.forbes.com/sites/lanceeliot/2025/01/15/embedding-llm-circuit-breakers-into-ai-might-save-us-from-a-whole-lot-of-ghastly-troubles/" target="_blank" rel="noopener noreferrer">Forbes</a>, <a href="https://www.lefigaro.fr/international/quand-chatgpt-deraille-faut-il-s-en-inquieter-20250201" target="_blank" rel="noopener noreferrer">Le Figaro</a>, and <a href="https://www.24heures.ch/chatgpt-quand-les-ia-deraillent-faut-il-sen-inquieter-117931493846" target="_blank" rel="noopener noreferrer">24 heures</a>.</strong> </td> </tr> <tr> <th scope="row">Jan 30, 2025</th> <td> <strong>Four papers accepted at ICLR 2025: <a href="https://arxiv.org/abs/2410.09024" target="_blank" rel="noopener noreferrer">AgentHarm</a>, <a href="https://arxiv.org/abs/2407.11969" target="_blank" rel="noopener noreferrer">past tense jailbreaks</a>, <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">adaptive jailbreaks</a>, and <a href="https://arxiv.org/abs/2405.19874" target="_blank" rel="noopener noreferrer">a comparison between in-context learning vs. instruction fine-tuning</a>.</strong> </td> </tr> <tr> <th scope="row">Dec 22, 2024</th> <td> Our paper <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</a> is featured on <a href="https://actu.epfl.ch/news/can-we-convince-ai-to-answer-harmful-requests/" target="_blank" rel="noopener noreferrer">EPFL’s main page</a>, in the largest Swiss newspapers (<a href="https://www.swissinfo.ch/eng/science/swiss-researchers-find-security-flaws-in-ai-models/88615042" target="_blank" rel="noopener noreferrer">SwissInfo</a>, <a href="https://www.letemps.ch/cyber/creation-de-bombes-cannibalisme-ou-cyberattaques-il-est-toujours-possible-de-demander-a-l-ia-des-conseils-sur-des-sujets-explosifs" target="_blank" rel="noopener noreferrer">Le Temps</a>, <a href="https://www.blick.ch/digital/epfl-forscher-knacken-sicherheitssperren-von-chatgpt-co-bomben-bauen-drogen-mischen-server-hacken-id20417778.html" target="_blank" rel="noopener noreferrer">Blick Digital</a>, <a href="https://www.blick.ch/fr/suisse/contenus-dangereux-lepfl-decouvre-des-failles-de-securite-dans-plusieurs-modeles-dia-id20427717.html" target="_blank" rel="noopener noreferrer">Blick Suisse</a>), and other media [<a href="https://techxplore.com/news/2024-12-convince-ai.html" target="_blank" rel="noopener noreferrer">1</a>, <a href="https://www.techexplorist.com/recent-large-language-models-remain-vulnerable-simple-manipulations/94951/" target="_blank" rel="noopener noreferrer">2</a>, <a href="https://www.miragenews.com/can-we-convince-ai-to-answer-harmful-requests-1382469/" target="_blank" rel="noopener noreferrer">3</a>, <a href="https://www.lemanbleu.ch/fr/Actualite/Economie/EPFL-des-failles-de-securite-dans-les-modeles-d-IA.html" target="_blank" rel="noopener noreferrer">4</a>, <a href="https://www.actuia.com/actualite/etude-epfl-les-limites-des-llms-face-aux-attaques-adaptatives/" target="_blank" rel="noopener noreferrer">5</a>]. </td> </tr> <tr> <th scope="row">Nov 4, 2024</th> <td> An invited talk at the UK AI Safety Institute about <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</a>, where we achieved 100% jailbreak success rate on all major LLMs, including GPT-4o and Claude 3.5 Sonnet. </td> </tr> <tr> <th scope="row">Oct 14, 2024</th> <td> <strong>Our new benchmark <a href="https://arxiv.org/abs/2410.09024" target="_blank" rel="noopener noreferrer">AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</a> is available online (collaboration between Gray Swan AI and UK AI Safety Institute).</strong> We need reliable evaluations for alignment of LLM agents equipped with external tools, especially in the adversarial setting. </td> </tr> <tr> <th scope="row">Sep 26, 2024</th> <td> <strong>Three papers accepted at NeurIPS 2024: <a href="https://arxiv.org/abs/2310.04415" target="_blank" rel="noopener noreferrer">Why Do We Need Weight Decay in Modern Deep Learning?</a>, <a href="https://arxiv.org/abs/2406.04313" target="_blank" rel="noopener noreferrer">Improving Alignment and Robustness with Circuit Breakers</a>, and <a href="https://arxiv.org/abs/2404.01318" target="_blank" rel="noopener noreferrer">JailbreakBench</a> (Datasets and Benchmarks Track).</strong> </td> </tr> <tr> <th scope="row">Jul 19, 2024</th> <td> Going to ICML 2024 in Vienna to present <a href="https://arxiv.org/abs/2402.04833" target="_blank" rel="noopener noreferrer">Long Is More for Alignment</a> at the main track and also <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">Adaptive Jailbreaking Attacks</a> and <a href="https://arxiv.org/abs/2404.01318" target="_blank" rel="noopener noreferrer">JailbreakBench</a> at the NextGenAISafety workshop. Feel free to ping me if you want to chat about robustness and generalization in LLMs! </td> </tr> <tr> <th scope="row">Jul 17, 2024</th> <td> <strong>Our new paper, <a href="https://arxiv.org/abs/2407.11969" target="_blank" rel="noopener noreferrer">Does Refusal Training in LLMs Generalize to the Past Tense?</a>, is available on arXiv now. See my <a href="https://twitter.com/maksym_andr/status/1813608842699079750" target="_blank" rel="noopener noreferrer">Twitter/X thread</a> for summary!</strong> </td> </tr> <tr> <th scope="row">Jun 7, 2024</th> <td> <strong>Incredibly excited about our new paper <a href="https://arxiv.org/abs/2406.04313" target="_blank" rel="noopener noreferrer">Improving Alignment and Robustness with Short Circuiting</a> (see the <a href="https://twitter.com/andyzou_jiaming/status/1799232319250743561" target="_blank" rel="noopener noreferrer">Twitter/X thread from Andy</a> for a summary)!</strong> Effective defenses against jailbreaking attacks on LLMs may be much more feasible than previously thought. </td> </tr> <tr> <th scope="row">May 31, 2024</th> <td> Our new paper <a href="https://arxiv.org/abs/2405.19874" target="_blank" rel="noopener noreferrer">Is In-Context Learning Sufficient for Instruction Following in LLMs?</a> is available online (see a <a href="https://twitter.com/maksym_andr/status/1796574290797604892" target="_blank" rel="noopener noreferrer">Twitter/X thread</a> for a summary). We study alignment of <em>base</em> models, including GPT-4-Base (!), via many-shot in-context learning. I.e., no fine-tuning whatsoever, just prompting - <em>how far can we go?</em> Check the paper for more details. </td> </tr> <tr> <th scope="row">May 2, 2024</th> <td> Our recent paper <a href="https://arxiv.org/abs/2402.04833" target="_blank" rel="noopener noreferrer">Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning</a> is accepted at ICML 2024! See <a href="https://twitter.com/maksym_andr/status/1785962403680633063" target="_blank" rel="noopener noreferrer">this Twitter/X thread</a> for a follow-up discussion. And see you in Vienna! </td> </tr> <tr> <th scope="row">Apr 2, 2024</th> <td> <strong>Our new paper <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</a> is available online (see a <a href="https://twitter.com/maksym_andr/status/1775877106422951938" target="_blank" rel="noopener noreferrer">Twitter/X thread</a> for a summary). We show how to jailbreak basically <em>all</em> leading safety-aligned LLMs with ≈100% success rate.</strong> </td> </tr> <tr> <th scope="row">Mar 28, 2024</th> <td> <strong>Our new benchmark <a href="https://arxiv.org/abs/2404.02151" target="_blank" rel="noopener noreferrer">JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</a> is available online (see a <a href="https://twitter.com/patrickrchao/status/1775567668520616060" target="_blank" rel="noopener noreferrer">Twitter/X thread</a> for a summary). We prioritize reproducibility, support adaptive attacks, and test-time defenses.</strong> </td> </tr> <tr> <th scope="row">Feb 15, 2024</th> <td> A talk at the <a href="https://www.mis.mpg.de/events/event/a-modern-look-at-the-relationship-between-sharpness-and-generalization" target="_blank" rel="noopener noreferrer">Math Machine Learning seminar MPI MIS + UCLA</a> about our paper <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a>. Slides: <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pdf">pdf</a>, <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Feb 7, 2024</th> <td> <strong>Our new paper <a href="https://arxiv.org/abs/2402.04833" target="_blank" rel="noopener noreferrer">Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning</a> is available online (see a <a href="https://twitter.com/maksym_andr/status/1755636355537715564" target="_blank" rel="noopener noreferrer">Twitter/X thread</a> for a summary). We need <em>simple</em> methods to better understand alignment.</strong> <div style="text-align: center;"> <img src="./assets/img/publication_preview/longismore_summary_slide.png" width="85%"> </div> </td> </tr> <tr> <th scope="row">Jan 16, 2024</th> <td> <a href="https://arxiv.org/abs/2307.06966" target="_blank" rel="noopener noreferrer">Layer-wise Linear Mode Connectivity</a> got accepted to ICLR 2024! </td> </tr> <tr> <th scope="row">Jan 5, 2024</th> <td> A talk at the <a href="https://mlcollective.org/dlct/" target="_blank" rel="noopener noreferrer">Deep Learning: Classics and Trends</a> (organized by <a href="https://mlcollective.org/" target="_blank" rel="noopener noreferrer">ML Collective</a>) about our recent work <a href="https://arxiv.org/abs/2310.04415" target="_blank" rel="noopener noreferrer">Why Do We Need Weight Decay in Modern Deep Learning?</a> (<a href="assets/pdf/Why%20Do%20We%20Need%20Weight%20Decay%20in%20Modern%20Deep%20Learning.pdf">slides</a>) </td> </tr> <tr> <th scope="row">Dec 21, 2023</th> <td> <strong>A new short paper <a href="https://www.andriushchenko.me/gpt4adv.pdf" target="_blank" rel="noopener noreferrer">Adversarial Attacks on GPT-4 via Simple Random Search</a> on how we can leverage <a href="https://twitter.com/OpenAIDevs/status/1735730662362189872" target="_blank" rel="noopener noreferrer">logprobs</a> for a black-box attack on the latest GPT-4-turbo (see a <a href="https://twitter.com/maksym_andr/status/1737844601891983563" target="_blank" rel="noopener noreferrer">Twitter/X thread</a> for a summary).</strong> <div style="text-align: center;"> <img src="./assets/img/publication_preview/gpt4adv.png" width="80%"> </div> </td> </tr> <tr> <th scope="row">Dec 10, 2023</th> <td> <strong>Going to NeurIPS’23 in New Orleans. Feel free to ping me if you want to chat!</strong> </td> </tr> <tr> <th scope="row">Nov 14, 2023</th> <td> A talk at the <a href="https://dlo-seminar.github.io/" target="_blank" rel="noopener noreferrer">Deep Learning and Optimization Seminar</a> (organized by faculties from Westlake University, City University of Hong Kong, Peking University) about our recent work <a href="https://arxiv.org/abs/2310.04415" target="_blank" rel="noopener noreferrer">Why Do We Need Weight Decay in Modern Deep Learning?</a> </td> </tr> <tr> <th scope="row">Nov 9, 2023</th> <td> A talk at the University of Tübingen about our recent work <a href="https://arxiv.org/abs/2310.04415" target="_blank" rel="noopener noreferrer">Why Do We Need Weight Decay in Modern Deep Learning?</a> </td> </tr> <tr> <th scope="row">Oct 30, 2023</th> <td> A talk at the <a href="https://sites.google.com/view/efficientml" target="_blank" rel="noopener noreferrer">Efficient ML</a> Reading Group (organized by TU Graz) about our recent work <a href="https://arxiv.org/abs/2310.04415" target="_blank" rel="noopener noreferrer">Why Do We Need Weight Decay in Modern Deep Learning?</a> </td> </tr> <tr> <th scope="row">Oct 23, 2023</th> <td> <strong>Excited to have participated in red teaming of OpenAI models as an external expert!</strong> I hope my findings will help improving the safety of their models/services. </td> </tr> <tr> <th scope="row">Oct 9, 2023</th> <td> <strong>Our new paper <a href="https://arxiv.org/abs/2310.04415" target="_blank" rel="noopener noreferrer">Why Do We Need Weight Decay in Modern Deep Learning?</a> is available online.</strong> Also check out our new preprint on <a href="https://arxiv.org/abs/2307.06966" target="_blank" rel="noopener noreferrer">layer-wise linear mode connectivity</a>. <div style="text-align: center;"> <img src="./assets/img/publication_preview/wd_summary_slide.png" width="85%"> </div> </td> </tr> <tr> <th scope="row">Sep 21, 2023</th> <td> <strong>Both <a href="https://arxiv.org/abs/2305.16292" target="_blank" rel="noopener noreferrer">Sharpness-Aware Minimization Leads to Low-Rank Features</a> and <a href="https://arxiv.org/abs/2306.04064" target="_blank" rel="noopener noreferrer">Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings</a> got accepted to NeurIPS 2023! See y’all in New Orleans! 🎶🎷</strong> </td> </tr> <tr> <th scope="row">Aug 23, 2023</th> <td> A talk at the <em><a href="https://groups.google.com/g/ellis-mathematics-of-deep-learning/" target="_blank" rel="noopener noreferrer">ELLIS Mathematics of Deep Learning</a></em> reading group about our ICML 2023 paper <a href="https://arxiv.org/abs/2210.05337" target="_blank" rel="noopener noreferrer">SGD with Large Step Sizes Learns Sparse Features</a>. Slides: <a href="../assets/pdf/SGD%20sparse%20features%20-%20ELLIS%20Mathematics%20of%20Deep%20Learning.pdf">pdf</a>, <a href="../assets/pdf/SGD%20sparse%20features%20-%20ELLIS%20Mathematics%20of%20Deep%20Learning.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Jul 23, 2023</th> <td> Going to ICML 2023 in Hawaii to present <a href="https://arxiv.org/abs/2210.05337" target="_blank" rel="noopener noreferrer">SGD with Large Step Sizes Learns Sparse Features</a> and <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A Modern Look at the Relationship Between Sharpness and Generalization</a> at the main track and <a href="https://arxiv.org/abs/2305.16292" target="_blank" rel="noopener noreferrer">Sharpness-Aware Minimization Leads to Low-Rank Features</a> at a <a href="https://sites.google.com/view/hidimlearning/" target="_blank" rel="noopener noreferrer">workshop</a>. Feel free to ping me if you want to chat! </td> </tr> <tr> <th scope="row">Jul 21, 2023</th> <td> A talk at the <strong><a href="https://thashim.github.io/" target="_blank" rel="noopener noreferrer">Tatsu’s lab</a> group meeting at Stanford</strong> about our ICML 2023 paper <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a>. Slides: <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20Stanford.pdf">pdf</a>, <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20Stanford.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Jun 5, 2023</th> <td> A talk at the <a href="https://sites.google.com/view/efficientml" target="_blank" rel="noopener noreferrer">Efficient ML</a> Reading Group (organized by TU Graz) about our ICML 2023 paper <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a>. Slides: <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pdf">pdf</a>, <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">May 30, 2023</th> <td> A talk at a mini-symposium of the <a href="https://jahrestagung.gamm-ev.de/" target="_blank" rel="noopener noreferrer">93rd Annual Meeting of the International Association of Applied Mathematics and Mechanics</a> about our <a href="https://arxiv.org/abs/2206.06232" target="_blank" rel="noopener noreferrer">ICML 2022</a> and <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">ICML 2023</a> papers on robustness/flatness in the parameter space. </td> </tr> <tr> <th scope="row">May 26, 2023</th> <td> <strong>Our new paper <a href="https://arxiv.org/abs/2305.16292" target="_blank" rel="noopener noreferrer">Sharpness-Aware Minimization Leads to Low-Rank Features</a> is available online!</strong> We investigate the low-rank effect of SAM which occurs in a variety of settings (regression, classification, contrastive learning) and architectures (MLPs, CNNs, Transformers). <div style="text-align: center;"> <img src="./assets/img/publication_preview/sam_low_rank_summary.png" width="80%"> </div> </td> </tr> <tr> <th scope="row">May 5, 2023</th> <td> A talk at the Amazon Research Reading Group about our ICML 2023 paper <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a>. Slides: <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pdf">pdf</a>, <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Apr 25, 2023</th> <td> <strong>Both <a href="https://arxiv.org/abs/2210.05337" target="_blank" rel="noopener noreferrer">SGD with large step sizes learns sparse features</a> and <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a> got accepted to ICML 2023! See you in Hawaii! 🌴</strong> </td> </tr> <tr> <th scope="row">Apr 12, 2023</th> <td> A talk at the <a href="https://dlo-seminar.github.io/" target="_blank" rel="noopener noreferrer">Deep Learning and Optimization Seminar</a> (organized by faculties from Westlake University, City University of Hong Kong, and Peking University) about our paper <a href="https://arxiv.org/abs/2210.05337" target="_blank" rel="noopener noreferrer">SGD with large step sizes learns sparse features</a>. Slides: <a href="assets/pdf/SGD%20sparse%20features%20-%20Westlake%20University.pdf">pdf</a>, <a href="assets/pdf/SGD%20sparse%20features%20-%20Westlake%20University.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Mar 13, 2023</th> <td> A talk at the OOD Robustness + Generalization Reading Group at CMU about our paper <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a>. Slides: <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pdf">pdf</a>, <a href="assets/pdf/A%20modern%20look%20at%20the%20relationship%20between%20sharpness%20and%20generalization%20-%20CMU%20reading%20group.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Feb 15, 2023</th> <td> <strong>Our new paper <a href="https://arxiv.org/abs/2302.07011" target="_blank" rel="noopener noreferrer">A modern look at the relationship between sharpness and generalization</a> is available online!</strong> Do flatter minima generalize better? Well, not really. <div style="text-align: center;"> <img src="./assets/img/publication_preview/sharpness_vs_generalization.png" alt="Sharpness-vs-generalization summary" width="85%"> </div> </td> </tr> <tr> <th scope="row">Dec 9, 2022</th> <td> A talk at the University of Luxembourg about our work with Adobe: <a href="https://arxiv.org/abs/2202.12860" target="_blank" rel="noopener noreferrer">ARIA: Adversarially Robust Image Attribution for Content Provenance</a>. </td> </tr> <tr> <th scope="row">Dec 1, 2022</th> <td> A talk in the <a href="http://www.matlog.net/" target="_blank" rel="noopener noreferrer">ML and Simulation Science Lab</a> of the University of Stuttgart about <a href="https://arxiv.org/abs/2010.09670" target="_blank" rel="noopener noreferrer">RobustBench</a> and <a href="https://arxiv.org/abs/2210.05337" target="_blank" rel="noopener noreferrer">SGD with large step sizes learns sparse features</a>. </td> </tr> <tr> <th scope="row">Nov 28, 2022</th> <td> <strong>Going to NeurIPS’22 in New Orleans. Feel free to ping me if you want to chat!</strong> </td> </tr> <tr> <th scope="row">Oct 28, 2022</th> <td> A talk at the <em><a href="https://groups.google.com/g/ellis-mathematics-of-deep-learning/" target="_blank" rel="noopener noreferrer">ELLIS Mathematics of Deep Learning</a></em> reading group about our ICML’22 paper <a href="https://arxiv.org/abs/2206.06232" target="_blank" rel="noopener noreferrer">Towards Understanding Sharpness-Aware Minimization</a>. Slides: <a href="assets/pdf/Understanding%20SAM%20-%20ELLIS%20Mathematics%20of%20Deep%20Learning.pdf">pdf</a>, <a href="assets/pdf/Understanding%20SAM%20-%20ELLIS%20Mathematics%20of%20Deep%20Learning.pptx">pptx</a>. </td> </tr> <tr> <th scope="row">Oct 12, 2022</th> <td> <strong>Our paper <a href="https://arxiv.org/abs/2210.05337" target="_blank" rel="noopener noreferrer">SGD with large step sizes learns sparse features</a> is available online!</strong> TL;DR: loss stabilization achieved via SGD with large step sizes leads to a hidden dynamics that promotes sparse feature learning. Also see <a href="https://twitter.com/maksym_andr/status/1580564569436524544" target="_blank" rel="noopener noreferrer">this twitter thread</a> for a quick summary of the main ideas. <div align="center"> <img src="./assets/img/publication_preview/sgd_sparse_features_fig1.png" alt="Summary" width="65%" style="vertical-align: bottom"> <video width="34%" src="https://user-images.githubusercontent.com/14852704/195183184-dca5111c-2093-429e-816f-ce25b4c3e2a0.mp4" loop="true" autoplay="autoplay" controls="" muted="" style="vertical-align: bottom"></video> </div> </td> </tr> <tr> <th scope="row">Oct 8, 2022</th> <td> Recognized as one of the <a href="https://neurips.cc/Conferences/2022/ProgramCommittee" target="_blank" rel="noopener noreferrer">top reviewers</a> at NeurIPS’22. Yay! 🎉 </td> </tr> <tr> <th scope="row">Sep 7, 2022</th> <td> A talk at Machine Learning Security Seminar hosted by University of Cagliari about our paper <a href="https://arxiv.org/abs/2202.12860" target="_blank" rel="noopener noreferrer">ARIA: Adversarially Robust Image Attribution for Content Provenance</a> (available on <a href="https://www.youtube.com/watch?v=JgWrDeUOUtk" target="_blank" rel="noopener noreferrer">youtube</a>). </td> </tr> <tr> <th scope="row">Sep 1, 2022</th> <td> <strong>Truly excited to be selected for the <a href="https://blog.google/technology/research/announcing-the-2022-phd-fellows/" target="_blank" rel="noopener noreferrer">Google PhD fellowship</a> and <a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/" target="_blank" rel="noopener noreferrer">OpenPhil AI fellowship</a>!</strong> </td> </tr> <tr> <th scope="row">Jun 13, 2022</th> <td> Our paper <a href="https://arxiv.org/abs/2206.06232" target="_blank" rel="noopener noreferrer">Towards Understanding Sharpness-Aware Minimization</a> got accepted to <strong>ICML’22</strong>! <div style="text-align: center;"> <img src="./assets/img/publication_preview/sam_paper.png" alt="SAM summary" width="85%"> </div> </td> </tr> <tr> <th scope="row">Apr 1, 2022</th> <td> Our paper <a href="https://arxiv.org/abs/2202.12860" target="_blank" rel="noopener noreferrer">ARIA: Adversarially Robust Image Attribution for Content Provenance</a> is accepted to the <a href="https://sites.google.com/view/mediaforensics2022/home" target="_blank" rel="noopener noreferrer">CVPR’22 Workshop on Media Forensics</a>. One of (a few?) applications where \(\ell_p\) adversarial robustness is well-motivated from the security point of view. <div style="text-align: center;"> <img src="./assets/img/publication_preview/aria_paper.png" alt="ARIA summary" width="85%"> </div> </td> </tr> <tr> <th scope="row">Mar 25, 2021</th> <td> A talk at the <a href="https://nlp-club.grammarly.ai/on-the-stability-of-fine-tuning-bert/" target="_blank" rel="noopener noreferrer">NLP club of Grammarly</a> about our paper <a href="https://arxiv.org/abs/2006.04884" target="_blank" rel="noopener noreferrer">On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a> (available on <a href="https://www.youtube.com/watch?v=VpDN9Db9ikQ" target="_blank" rel="noopener noreferrer">youtube</a>). </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>