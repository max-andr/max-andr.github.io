---
layout: post
date: 2022-10-12 00:00:01+0200
inline: true
---

Our paper **[SGD with large step sizes learns sparse features](https://arxiv.org/abs/2210.05337)** is available online! TL;DR: loss stabilization achieved via SGD with large step sizes leads to a hidden dynamics that promotes sparse feature learning. Also see [this twitter thread](https://twitter.com/maksym_andr/status/1580564569436524544) for a quick summary of the main ideas.

<div align="center">
  <img src="./assets/img/publication_preview/sgd_sparse_features_fig1.png" alt="Summary" width="65%" style="vertical-align: bottom">
  <video width="34%" src="https://user-images.githubusercontent.com/14852704/195183184-dca5111c-2093-429e-816f-ce25b4c3e2a0.mp4" loop="true" autoplay="autoplay" controls muted  style="vertical-align: bottom"></video>
</div>

<!-- <div style="text-align: center;">
  <img src="./assets/img/publication_preview/sam_paper.png" alt="SAM summary" width="95%"/>
</div> -->
<!-- <p align="center"><img src="./assets/img/publication_preview/twitter.gif" width="500" /></p> -->
<!-- <p align="center"><video src="./assets/img/publication_preview/twitter.mp4" controls="controls" style="max-width: 500px;"></video></p> -->
<!-- <p align="center"><video width="400px" src="https://user-images.githubusercontent.com/14852704/195183184-dca5111c-2093-429e-816f-ce25b4c3e2a0.mp4" controls="controls" loop="true" autoplay="autoplay" controls muted></video></p> -->

